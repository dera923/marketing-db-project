# scripts/etl_pipeline.py
"""
Polarsã‚’ä½¿ç”¨ã—ãŸé«˜é€ŸETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿå‡¦ç†
- æ¬ æå€¤è£œå®Œãƒ»å‹å¤‰æ›ãƒ»é›†è¨ˆ
- CSV/ParquetåŒæ–¹å‘å¯¾å¿œ
"""

import polars as pl
import pandas as pd
from pathlib import Path
import sys
import argparse
from typing import Optional
import logging
from datetime import datetime

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def read_data(file_path: str) -> pl.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆCSV/Parquetå¯¾å¿œï¼‰"""
    logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    
    if file_path.endswith('.csv'):
        df = pl.read_csv(file_path)
    elif file_path.endswith('.parquet'):
        df = pl.read_parquet(file_path)
    else:
        raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ï¼ˆCSV/Parquetã®ã¿ï¼‰")
    
    logger.info(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {df.height:,}è¡Œ Ã— {df.width}åˆ—")
    return df

def clean_data(df: pl.DataFrame) -> pl.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    logger.info("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    
    initial_rows = df.height
    
    # æ¬ æå€¤å‡¦ç†
    df_cleaned = df.drop_nulls()
    
    # é‡è¤‡é™¤å»
    df_cleaned = df_cleaned.unique()
    
    # åŸºæœ¬çš„ãªå‹å¤‰æ›ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if 'amount' in df.columns:
        # è² ã®é‡‘é¡ã‚’0ã«ä¿®æ­£
        df_cleaned = df_cleaned.with_columns([
            pl.when(pl.col('amount') < 0)
            .then(0)
            .otherwise(pl.col('amount'))
            .alias('amount')
        ])
    
    # æ—¥ä»˜åˆ—ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
    date_cols = [col for col in df.columns if 'date' in col.lower() or col in ['created_at', 'updated_at']]
    for col in date_cols:
        try:
            df_cleaned = df_cleaned.with_columns([
                pl.col(col).str.to_datetime().alias(col)
            ])
        except:
            logger.warning(f"âš ï¸  æ—¥ä»˜å¤‰æ›ã«å¤±æ•—: {col}")
    
    removed_rows = initial_rows - df_cleaned.height
    logger.info(f"âœ… ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {removed_rows:,}è¡Œã‚’é™¤å»")
    
    return df_cleaned

def transform_data(df: pl.DataFrame) -> pl.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»é›†è¨ˆ"""
    logger.info("âš¡ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
    
    df_transformed = df
    
    # é‡‘é¡é–¢é€£ã®å¤‰æ›
    if 'amount' in df.columns:
        df_transformed = df_transformed.with_columns([
            # é‡‘é¡ã®ãƒ­ã‚°å¤‰æ›
            (pl.col('amount') + 1).log().alias('amount_log'),
            # é‡‘é¡ã®æ¨™æº–åŒ–
            ((pl.col('amount') - pl.col('amount').mean()) / pl.col('amount').std()).alias('amount_normalized')
        ])
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥é›†è¨ˆï¼ˆuser_idãŒã‚ã‚‹å ´åˆï¼‰
    if 'user_id' in df.columns and 'amount' in df.columns:
        user_agg = df.group_by('user_id').agg([
            pl.col('amount').sum().alias('user_total_amount'),
            pl.col('amount').mean().alias('user_avg_amount'),
            pl.col('amount').count().alias('user_transaction_count'),
            pl.col('amount').max().alias('user_max_amount')
        ])
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
        df_transformed = df_transformed.join(user_agg, on='user_id', how='left')
    
    # æ—¥ä»˜é–¢é€£ã®ç‰¹å¾´é‡ç”Ÿæˆ
    if 'created_at' in df.columns:
        df_transformed = df_transformed.with_columns([
            pl.col('created_at').dt.year().alias('year'),
            pl.col('created_at').dt.month().alias('month'),
            pl.col('created_at').dt.weekday().alias('weekday'),
            pl.col('created_at').dt.hour().alias('hour')
        ])
    
    new_cols = df_transformed.width - df.width
    logger.info(f"âœ… å¤‰æ›å®Œäº†: {new_cols}åˆ—ã‚’è¿½åŠ ")
    
    return df_transformed

def save_data(df: pl.DataFrame, file_path: str) -> None:
    """ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆCSV/Parquetå¯¾å¿œï¼‰"""
    logger.info(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­: {file_path}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    output_dir = Path(file_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if file_path.endswith('.csv'):
        df.write_csv(file_path)
    elif file_path.endswith('.parquet'):
        df.write_parquet(file_path)
    else:
        raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å‡ºåŠ›å½¢å¼ã§ã™ï¼ˆCSV/Parquetã®ã¿ï¼‰")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
    file_size = Path(file_path).stat().st_size / (1024 * 1024)  # MB
    logger.info(f"âœ… ä¿å­˜å®Œäº†: {file_path} ({file_size:.2f}MB)")

def run_etl_pipeline(input_path: str, output_path: str) -> dict:
    """ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    start_time = datetime.now()
    logger.info("ğŸš€ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™")
    
    try:
        # Extract: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = read_data(input_path)
        
        # Transform: ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        df_cleaned = clean_data(df)
        df_transformed = transform_data(df_cleaned)
        
        # Load: ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        save_data(df_transformed, output_path)
        
        # å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        summary = {
            'input_file': input_path,
            'output_file': output_path,
            'input_rows': df.height,
            'output_rows': df_transformed.height,
            'input_columns': df.width,
            'output_columns': df_transformed.width,
            'processing_time_seconds': processing_time,
            'rows_removed': df.height - df_transformed.height,
            'columns_added': df_transformed.width - df.width
        }
        
        logger.info(f"ğŸŠ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
        logger.info(f"ğŸ“Š å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        logger.info(f"ğŸ“ˆ {summary['input_rows']:,}è¡Œ â†’ {summary['output_rows']:,}è¡Œ")
        logger.info(f"ğŸ“‹ {summary['input_columns']}åˆ— â†’ {summary['output_columns']}åˆ—")
        
        return summary
        
    except Exception as e:
        logger.error(f"âŒ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        raise

def compare_with_pandas(file_path: str) -> None:
    """Pandas vs Polarsæ€§èƒ½æ¯”è¼ƒ"""
    if not Path(file_path).exists():
        logger.warning("âš ï¸  æ¯”è¼ƒç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    logger.info("ğŸƒâ€â™‚ï¸ Pandas vs Polars æ€§èƒ½æ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")
    
    import time
    
    # Pandaså‡¦ç†æ™‚é–“æ¸¬å®š
    start_time = time.time()
    df_pandas = pd.read_csv(file_path)
    df_pandas = df_pandas.dropna().drop_duplicates()
    if 'amount' in df_pandas.columns:
        df_pandas['amount_log'] = (df_pandas['amount'] + 1).apply(lambda x: x.__log__())
    pandas_time = time.time() - start_time
    
    # Polarså‡¦ç†æ™‚é–“æ¸¬å®š
    start_time = time.time()
    df_polars = (pl.read_csv(file_path)
                .drop_nulls()
                .unique())
    if 'amount' in df_polars.columns:
        df_polars = df_polars.with_columns([
            (pl.col('amount') + 1).log().alias('amount_log')
        ])
    polars_time = time.time() - start_time
    
    speed_improvement = pandas_time / polars_time if polars_time > 0 else float('inf')
    
    logger.info(f"âš¡ æ€§èƒ½æ¯”è¼ƒçµæœ:")
    logger.info(f"   Pandas: {pandas_time:.3f}ç§’")
    logger.info(f"   Polars: {polars_time:.3f}ç§’")
    logger.info(f"   é«˜é€ŸåŒ–: {speed_improvement:.1f}å€")

if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(
        description='é«˜é€ŸETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆPolarsä½¿ç”¨ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python scripts/etl_pipeline.py --input data/raw/sales.csv --output data/processed/sales.parquet
  python scripts/etl_pipeline.py -i data/raw/users.csv -o data/processed/users.parquet --compare
        """
    )
    
    parser.add_argument(
        '--input', '-i', 
        required=True, 
        help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.csv ã¾ãŸã¯ .parquet)'
    )
    parser.add_argument(
        '--output', '-o', 
        required=True, 
        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.csv ã¾ãŸã¯ .parquet)'
    )
    parser.add_argument(
        '--compare', 
        action='store_true',
        help='Pandas vs Polarsæ€§èƒ½æ¯”è¼ƒã‚’å®Ÿè¡Œ'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º'
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        summary = run_etl_pipeline(args.input, args.output)
        
        # æ€§èƒ½æ¯”è¼ƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if args.compare:
            compare_with_pandas(args.input)
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*50)
        print("ğŸŠ ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†ï¼")
        print("="*50)
        print(f"å…¥åŠ›: {summary['input_file']}")
        print(f"å‡ºåŠ›: {summary['output_file']}")
        print(f"å‡¦ç†æ™‚é–“: {summary['processing_time_seconds']:.2f}ç§’")
        print(f"ãƒ‡ãƒ¼ã‚¿å¤‰æ›: {summary['input_rows']:,}è¡Œ â†’ {summary['output_rows']:,}è¡Œ")
        print(f"åˆ—è¿½åŠ : +{summary['columns_added']}åˆ—")
        print("="*50)
        
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
